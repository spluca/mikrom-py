# Docker Build Optimization

Este documento explica las optimizaciones realizadas al Dockerfile para reducir dr√°sticamente el tiempo de compilaci√≥n.

## üìä Resultados de Rendimiento

### Comparaci√≥n: Alpine vs Debian Trixie

| M√©trica | Alpine (Anterior) | Debian Trixie (Actual) | Mejora |
|---------|-------------------|------------------------|--------|
| **Build inicial (sin cach√©)** | ~10-15 min | **6:42 min** | ~40% m√°s r√°pido |
| **Rebuild completo (con cach√©)** | ~2-3 min | **1.3 segundos** | **99% m√°s r√°pido** |
| **Rebuild tras cambio de c√≥digo** | ~1-2 min | **0.5 segundos** | **99.5% m√°s r√°pido** |
| **Tama√±o de imagen** | ~120-150 MB | **115 MB** | Similar |
| **Compilaci√≥n de Rust/Cargo** | ‚úÖ Requerida | ‚ùå No requerida | Eliminada |
| **Wheels precompilados** | ‚ùå No disponibles | ‚úÖ Disponibles | S√≠ |

### Impacto Real

- **Desarrollo**: Cambios en c√≥digo se construyen en **menos de 1 segundo** üöÄ
- **CI/CD**: Builds m√°s r√°pidos = despliegues m√°s r√°pidos
- **Cache efectivo**: Dependencias solo se reinstalan si cambia `pyproject.toml` o `uv.lock`

## üîç Problemas del Dockerfile Anterior (Alpine)

### 1. Compilaci√≥n de Rust/Cargo (Problema Principal)

**C√≥digo problem√°tico:**
```dockerfile
FROM python:3.14-alpine
RUN apk add --no-cache \
    cargo \
    rust \
    gcc \
    musl-dev \
    postgresql-dev \
    libffi-dev \
    openssl-dev
```

**Problemas:**
- **Rust y Cargo** toman 5-15 minutos en compilar paquetes de criptograf√≠a
- Paquetes como `cryptography`, `gevent`, `psycopg2-binary` se compilan desde el c√≥digo fuente
- Alpine usa `musl` en lugar de `glibc`, forzando compilaci√≥n de muchos paquetes

### 2. Sin Cach√© Efectivo de Dependencias

**C√≥digo problem√°tico:**
```dockerfile
COPY pyproject.toml uv.lock ./
RUN uv pip install --no-cache -r pyproject.toml
COPY . .  # ‚ö†Ô∏è Cualquier cambio de c√≥digo invalida TODO el cach√©
```

**Problemas:**
- Flag `--no-cache` descarta paquetes descargados
- Usar `pyproject.toml` directamente en lugar del lockfile
- Copiar c√≥digo demasiado temprano invalida capas de dependencias

### 3. Uso Incorrecto de uv

**C√≥digo problem√°tico:**
```dockerfile
RUN python3 -m venv /app/.venv && \
    /app/.venv/bin/pip install --upgrade pip && \
    . /app/.venv/bin/activate && \
    uv pip install --no-cache -r pyproject.toml
```

**Problemas:**
- No usa `uv sync` que aprovecha el lockfile
- Mezcla pip y uv innecesariamente
- No aprovecha el cach√© de uv

## ‚úÖ Soluciones Implementadas

### 1. Cambio de Alpine a Debian Trixie

**Nuevo c√≥digo:**
```dockerfile
FROM python:3.14-slim-trixie AS builder

# Solo dependencias m√≠nimas - la mayor√≠a son wheels precompilados
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*
```

**Beneficios:**
- ‚úÖ **Sin Rust/Cargo**: No se necesita compilar nada
- ‚úÖ **Wheels precompilados**: PyPI tiene wheels para glibc (Debian)
- ‚úÖ **Build 40% m√°s r√°pido**: De 10-15 min a 6-7 min
- ‚úÖ **Tama√±o similar**: 115 MB vs 120-150 MB de Alpine

### 2. Capas de Docker Optimizadas

**Nuevo c√≥digo:**
```dockerfile
# Stage 1: Builder - solo dependencias
COPY pyproject.toml uv.lock ./
RUN uv sync --frozen --no-dev --no-install-project

# Stage 2: Runtime - copia c√≥digo al final
COPY --from=builder /app/.venv /app/.venv
COPY --chown=appuser:appuser . .  # ‚úÖ C√≥digo copiado al FINAL
```

**Beneficios:**
- ‚úÖ **Cach√© efectivo**: Dependencias en capas separadas
- ‚úÖ **Cambios de c√≥digo r√°pidos**: Solo se reconstruye la √∫ltima capa
- ‚úÖ **Rebuilds en ~1 segundo** cuando solo cambia c√≥digo

### 3. Uso Correcto de uv

**Nuevo c√≥digo:**
```dockerfile
RUN uv sync --frozen --no-dev --no-install-project
```

**Beneficios:**
- ‚úÖ **`uv sync`**: Usa el lockfile `uv.lock` para reproducibilidad exacta
- ‚úÖ **`--frozen`**: Falla si lockfile est√° desactualizado (seguridad)
- ‚úÖ **`--no-dev`**: Excluye dependencias de desarrollo
- ‚úÖ **`--no-install-project`**: Solo instala dependencias, no el proyecto
- ‚úÖ **Cach√© interno de uv**: Reutiliza paquetes descargados

### 4. Multi-stage Build Optimizado

**Nuevo c√≥digo:**
```dockerfile
# Stage 1: Builder (con herramientas de compilaci√≥n)
FROM python:3.14-slim-trixie AS builder
RUN apt-get install gcc libpq-dev ...
RUN uv sync --frozen --no-dev --no-install-project

# Stage 2: Runtime (solo runtime, sin herramientas)
FROM python:3.14-slim-trixie
RUN apt-get install libpq5 curl ...  # Solo runtime
COPY --from=builder /app/.venv /app/.venv
```

**Beneficios:**
- ‚úÖ **Imagen final m√°s peque√±a**: Sin gcc, headers, etc.
- ‚úÖ **Seguridad**: Menos superficie de ataque
- ‚úÖ **Runtime limpio**: Solo lo necesario para ejecutar

## üéØ Cu√°ndo se Reconstruye Cada Capa

### Cambio de Dependencias (pyproject.toml o uv.lock)
```bash
# Tiempo: ~6-7 minutos (build completo)
# Se reconstruye: ‚úÖ Capa de dependencias + ‚úÖ Capa de c√≥digo
```

### Cambio de C√≥digo (solo archivos .py)
```bash
# Tiempo: ~0.5-1 segundo (solo √∫ltima capa)
# Se reconstruye: ‚ùå Capa de dependencias + ‚úÖ Capa de c√≥digo
```

### Sin Cambios
```bash
# Tiempo: ~1 segundo (todo en cach√©)
# Se reconstruye: ‚ùå Nada
```

## üìù Comandos de Build

### Build Normal
```bash
docker build -t mikrom-py:latest .
# Tiempo: ~1 segundo (con cach√©) / ~7 min (sin cach√©)
```

### Build sin Cach√© (Benchmark)
```bash
docker build --no-cache -t mikrom-py:latest .
# Tiempo: ~6:42 minutos
```

### Build con BuildKit (m√°s r√°pido)
```bash
DOCKER_BUILDKIT=1 docker build -t mikrom-py:latest .
# Usa cach√© distribuido y builds en paralelo
```

### Verificar Tama√±o de Imagen
```bash
docker images mikrom-py:latest
# Tama√±o: ~115 MB (comprimido), ~662 MB (disco)
```

## üîß Configuraci√≥n Recomendada

### Para Desarrollo Local
```bash
# Use docker-compose para builds autom√°ticos
docker-compose up --build
```

### Para CI/CD
```yaml
# .github/workflows/docker.yml
- name: Build Docker Image
  run: |
    docker build \
      --cache-from=mikrom-py:latest \
      --tag=mikrom-py:${{ github.sha }} \
      .
```

### Para Producci√≥n
```bash
# Multi-platform build
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  --tag mikrom-py:latest \
  --push .
```

## üöÄ Mejoras Adicionales Opcionales

### 1. Cach√© de Docker Registry
```dockerfile
# Usar cach√© remoto para CI/CD
RUN --mount=type=cache,target=/root/.cache/uv \
    uv sync --frozen --no-dev --no-install-project
```

### 2. Optimizaci√≥n de Dependencias
```bash
# Revisar dependencias innecesarias
uv pip list --outdated
uv pip tree  # Ver √°rbol de dependencias
```

### 3. Imagen Base Distroless
```dockerfile
# Para producci√≥n ultra-segura
FROM gcr.io/distroless/python3-debian12:nonroot
COPY --from=builder /app/.venv /app/.venv
```

## üìö Referencias

- [Docker Multi-stage Builds](https://docs.docker.com/build/building/multi-stage/)
- [uv Documentation](https://github.com/astral-sh/uv)
- [Python Wheels](https://pythonwheels.com/)
- [Debian Trixie](https://www.debian.org/releases/trixie/)

## üéØ Optimizaci√≥n de Imagen Compartida

### Problema: M√∫ltiples Servicios, Misma Imagen

Anteriormente, cada servicio (app, worker, beat, flower) constru√≠a su propia imagen id√©ntica:

```yaml
# ‚ùå Configuraci√≥n anterior (ineficiente)
app:
  build:
    context: .
    dockerfile: Dockerfile

worker:
  build:
    context: .
    dockerfile: Dockerfile

beat:
  build:
    context: .
    dockerfile: Dockerfile

flower:
  build:
    context: .
    dockerfile: Dockerfile
```

**Problemas:**
- üêå **Build 4√ó m√°s lento**: Construye 4 im√°genes id√©nticas
- üíæ **Desperdicio de espacio**: 4 √ó 662MB = 2.6GB
- üîÑ **Sin cache compartido**: Cada servicio reconstruye todo
- ‚è±Ô∏è **CI/CD m√°s lento**: M√°s tiempo de transferencia

### Soluci√≥n: Build Compartido

```yaml
# ‚úÖ Configuraci√≥n actual (optimizada)
app:
  build:
    context: .
    dockerfile: Dockerfile
  image: mikrom-py:latest  # Nombre expl√≠cito

worker:
  image: mikrom-py:latest  # Reutiliza imagen de app

beat:
  image: mikrom-py:latest  # Reutiliza imagen de app

flower:
  image: mikrom-py:latest  # Reutiliza imagen de app
```

**Beneficios:**
- ‚ö° **Build 4√ó m√°s r√°pido**: Solo construye 1 imagen
- üíæ **Ahorra ~2GB**: De 2.6GB a 662MB
- üîÑ **Cache compartido**: Todos usan la misma imagen
- üöÄ **M√°s eficiente**: Menos I/O de disco

### Healthcheck por Servicio

Cada servicio define su propio healthcheck en `docker-compose.yml`:

```yaml
app:
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]

worker:
  healthcheck:
    test: ["CMD-SHELL", "celery -A mikrom.celery_app inspect ping"]

beat:
  healthcheck:
    test: ["CMD-SHELL", "test -f /tmp/celerybeat-schedule"]

flower:
  healthcheck:
    test: ["CMD-SHELL", "curl -f http://127.0.0.1:5555/healthcheck"]
```

Por eso se **elimin√≥ el healthcheck del Dockerfile** - cada servicio necesita su propio check espec√≠fico.

### Comandos por Servicio

Cada servicio sobreescribe el CMD del Dockerfile:

```yaml
# app: usa el CMD por defecto del Dockerfile
# CMD ["python", "-m", "uvicorn", "mikrom.main:app", ...]

worker:
  command: celery -A mikrom.celery_app worker --pool=gevent --concurrency=100

beat:
  command: celery -A mikrom.celery_app beat --loglevel=info

flower:
  command: celery -A mikrom.celery_app flower --port=5555
```

### Resultados

| M√©trica | Antes | Despu√©s | Mejora |
|---------|-------|---------|--------|
| **Im√°genes en disco** | 2.6GB (4 imgs) | 662MB (1 img) | -75% |
| **Tiempo de build** | ~27 min (4√ó6:42) | ~7 min (1√ó6:42) | -74% |
| **Espacio ahorrado** | - | ~2GB | üì¶ |
| **Cache compartido** | ‚ùå No | ‚úÖ S√≠ | üöÄ |

## üéâ Conclusi√≥n

La combinaci√≥n de optimizaciones ha logrado:

### Migraci√≥n Alpine ‚Üí Debian Trixie
- ‚úÖ **99% m√°s r√°pido** en rebuilds
- ‚úÖ **40% m√°s r√°pido** en builds iniciales
- ‚úÖ **Sin compilaci√≥n de Rust/Cargo**

### Build Compartido
- ‚úÖ **75% menos espacio** en disco
- ‚úÖ **4√ó m√°s r√°pido** en docker compose build
- ‚úÖ **Cache compartido** entre servicios

### Resultado Final
- **Desarrollo**: Cambios de c√≥digo en **0.5 segundos** üöÄ
- **Builds completos**: De 27 minutos a **7 minutos** ‚ö°
- **Espacio en disco**: Ahorro de **~2GB** üíæ
- **Experiencia de desarrollo**: **Dram√°ticamente mejorada** üéØ

**Tiempo total ahorrado:**
- Antes: 27 min build completo (4 servicios), 1-2 min por cambio
- Ahora: 7 min build completo, **0.5 segundos** por cambio
- **Ahorro: ~75% en builds, ~99.5% en desarrollo iterativo** üöÄ
